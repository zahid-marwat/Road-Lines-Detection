{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784b7d23",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdff028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "coco_ann_path = os.path.join('data','coco','coco.json')\n",
    "\n",
    "images_dir = Path('data', 'images')\n",
    "\n",
    "output_dir = Path('data', 'coco', 'split')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output image folders\n",
    "train_img_dir = output_dir / \"train\"\n",
    "val_img_dir = output_dir / \"val\"\n",
    "test_img_dir = output_dir / \"test\"\n",
    "train_img_dir.mkdir(exist_ok=True)\n",
    "val_img_dir.mkdir(exist_ok=True)\n",
    "test_img_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_path = train_img_dir / \"train.json\"\n",
    "val_path = val_img_dir / \"val.json\"\n",
    "test_path = test_img_dir / \"test.json\"\n",
    "\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "with open(coco_ann_path, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "images = coco['images']\n",
    "random.shuffle(images)\n",
    "annotations = coco['annotations']\n",
    "categories = coco['categories']\n",
    "\n",
    "n = len(images)\n",
    "n_train = int(n * train_ratio)\n",
    "n_val = int(n * val_ratio)\n",
    "\n",
    "train_images = images[:n_train]\n",
    "val_images = images[n_train:n_train + n_val]\n",
    "test_images = images[n_train + n_val:]\n",
    "\n",
    "def filter_annotations(images_subset):\n",
    "    image_ids = set(img['id'] for img in images_subset)\n",
    "    return [ann for ann in annotations if ann['image_id'] in image_ids]\n",
    "\n",
    "splits = [\n",
    "    (train_path, train_images, train_img_dir),\n",
    "    (val_path, val_images, val_img_dir),\n",
    "    (test_path, test_images, test_img_dir)\n",
    "]\n",
    "\n",
    "for path, imgs, img_dir in splits:\n",
    "    anns = filter_annotations(imgs)\n",
    "    split_dict = {\n",
    "        \"images\": imgs,\n",
    "        \"annotations\": anns,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(split_dict, f)\n",
    "    # Copy images\n",
    "    for img in imgs:\n",
    "        src = images_dir / img['file_name']\n",
    "        dst = img_dir / img['file_name']\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "        else:\n",
    "            print(f\"Warning: {src} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98121e64",
   "metadata": {},
   "source": [
    "### Classes Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "with open('data/coco/coco.json', 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "categories = coco['categories']\n",
    "class_names = [cat['name'] for cat in categories]\n",
    "print(class_names)\n",
    "annotations = coco['annotations']\n",
    "cat_counter = Counter(ann['category_id'] for ann in annotations)\n",
    "for cat in categories:\n",
    "    print(f\"{cat['name']}: {cat_counter.get(cat['id'], 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d48a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18878169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm  # for Progress bar\n",
    "\n",
    "\n",
    "# ---------- Dataset Class ----------\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, annFile, transforms=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transforms = transforms\n",
    "        self.category_id_to_name = {cat['id']: cat['name'] for cat in self.coco.loadCats(self.coco.getCatIds())}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        img_path = os.path.join(self.root, path)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes, labels, masks = [], [], []\n",
    "        for ann in anns:\n",
    "            xmin, ymin, width, height = ann['bbox']\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(ann['category_id'])\n",
    "            masks.append(coco.annToMask(ann))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([img_id])\n",
    "        area = torch.as_tensor([ann['area'] for ann in anns], dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor([ann.get('iscrowd', 0) for ann in anns], dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "# ---------- Transforms ----------\n",
    "def get_transform(train):\n",
    "    return transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# ---------- Model ----------\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask, hidden_layer, num_classes\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train_model(\n",
    "    train_img_dir,\n",
    "    train_ann_file,\n",
    "    val_img_dir,\n",
    "    val_ann_file,\n",
    "    num_epochs=10,\n",
    "    batch_size=2,\n",
    "    model_output_path=\"maskrcnn_trained.pth\"\n",
    "):\n",
    "    train_dataset = CocoDataset(\n",
    "        root=train_img_dir,\n",
    "        annFile=train_ann_file,\n",
    "        transforms=get_transform(train=True)\n",
    "    )\n",
    "\n",
    "    val_dataset = CocoDataset(\n",
    "        root=val_img_dir,\n",
    "        annFile=val_ann_file,\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    num_classes = len(train_dataset.category_id_to_name) + 1  # +1 for background\n",
    "    model = get_model(num_classes)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        print(f\"\\nðŸŒ€ Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "        # Add progress bar using tqdm\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Training\", unit=\"batch\"):\n",
    "            new_images, new_targets = [], []\n",
    "            for img, tgt in zip(images, targets):\n",
    "                if tgt[\"boxes\"].numel() == 0:\n",
    "                    continue\n",
    "                new_images.append(img)\n",
    "                new_targets.append(tgt)\n",
    "\n",
    "            if len(new_images) == 0:\n",
    "                continue\n",
    "\n",
    "            images = [img.to(device) for img in new_images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in new_targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            epoch_loss += losses.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        print(f\" Epoch Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_output_path)\n",
    "    print(f\"\\n Training complete. Model saved to: {model_output_path}\")\n",
    "\n",
    "import os\n",
    "# ---------- Run Training ----------\n",
    "model_output_path = \"maskrcnn_resnet50_trained.pth\"\n",
    "path_to_dataset_main = r\"data\\coco\\split\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_to_dataset = path_to_dataset_main\n",
    "    train_img_dir = os.path.join(path_to_dataset, \"train\")\n",
    "    val_img_dir = os.path.join(path_to_dataset, \"val\")\n",
    "    train_ann_file = os.path.join(train_img_dir, \"train.json\")\n",
    "    val_ann_file = os.path.join(val_img_dir, \"val.json\")\n",
    "    train_model(\n",
    "        train_img_dir=train_img_dir,\n",
    "        train_ann_file=train_ann_file,\n",
    "        val_img_dir=val_img_dir,\n",
    "        val_ann_file=val_ann_file,\n",
    "        num_epochs=5,\n",
    "        batch_size=2,\n",
    "        model_output_path = model_output_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06062dba",
   "metadata": {},
   "source": [
    "### Evaluation of the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation Function ----------\n",
    "def evaluate_model(model, test_img_dir, test_ann_file, device):\n",
    "    test_dataset = CocoDataset(\n",
    "        root=test_img_dir,\n",
    "        annFile=test_ann_file,\n",
    "        transforms=get_transform(train=False)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    # Load COCO API for evaluation metrics\n",
    "    coco_gt = COCO(test_ann_file)\n",
    "    \n",
    "    # Initialize list to store COCO-format detections\n",
    "    coco_dt = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                image_id = target['image_id'].item()\n",
    "                boxes = output['boxes'].cpu()\n",
    "                scores = output['scores'].cpu()\n",
    "                labels = output['labels'].cpu()\n",
    "                \n",
    "                # Apply score threshold (e.g. 0.5)\n",
    "                score_threshold = 0.5\n",
    "                keep_idxs = torch.where(scores > score_threshold)[0]\n",
    "                \n",
    "                # Convert to COCO format (xmin, ymin, width, height)\n",
    "                for idx in keep_idxs:\n",
    "                    box = boxes[idx].tolist()\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    width = x2 - x1\n",
    "                    height = y2 - y1\n",
    "                    \n",
    "                    detection = {\n",
    "                        'image_id': image_id,\n",
    "                        'category_id': labels[idx].item(),\n",
    "                        'bbox': [x1, y1, width, height],\n",
    "                        'score': scores[idx].item()\n",
    "                    }\n",
    "                    coco_dt.append(detection)\n",
    "    \n",
    "    # If we have detections, evaluate using COCO metrics\n",
    "    if coco_dt:\n",
    "        # Convert detections to COCO format\n",
    "        coco_pred = coco_gt.loadRes(coco_dt)\n",
    "        \n",
    "        # Run COCO evaluation\n",
    "        coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        \n",
    "        # Extract evaluation metrics\n",
    "        ap_mean = coco_eval.stats[0]\n",
    "        ap50 = coco_eval.stats[1]\n",
    "        ap75 = coco_eval.stats[2]\n",
    "        ap_small = coco_eval.stats[3]\n",
    "        ap_medium = coco_eval.stats[4]\n",
    "        ap_large = coco_eval.stats[5]\n",
    "        ar_max1 = coco_eval.stats[6]\n",
    "        ar_max10 = coco_eval.stats[7]\n",
    "        ar_max100 = coco_eval.stats[8]\n",
    "        ar_small = coco_eval.stats[9]\n",
    "        ar_medium = coco_eval.stats[10]\n",
    "        ar_large = coco_eval.stats[11]\n",
    "        \n",
    "        print(f\"AP@0.5: {ap50:.4f}\")\n",
    "        \n",
    "        # Create a metrics table\n",
    "        metrics_table = [\n",
    "            [\"Metric\", \"Value\"],\n",
    "            [\"AP@0.50:0.95\", f\"{ap_mean:.4f}\"],\n",
    "            [\"AP@0.50\", f\"{ap50:.4f}\"],\n",
    "            [\"AP@0.75\", f\"{ap75:.4f}\"],\n",
    "            [\"AP (small)\", f\"{ap_small:.4f}\"],\n",
    "            [\"AP (medium)\", f\"{ap_medium:.4f}\"],\n",
    "            [\"AP (large)\", f\"{ap_large:.4f}\"],\n",
    "            [\"AR@maxDets=1\", f\"{ar_max1:.4f}\"],\n",
    "            [\"AR@maxDets=10\", f\"{ar_max10:.4f}\"],\n",
    "            [\"AR@maxDets=100\", f\"{ar_max100:.4f}\"],\n",
    "            [\"AR (small)\", f\"{ar_small:.4f}\"],\n",
    "            [\"AR (medium)\", f\"{ar_medium:.4f}\"],\n",
    "            [\"AR (large)\", f\"{ar_large:.4f}\"]\n",
    "        ]\n",
    "        \n",
    "        # Print metrics table\n",
    "        print(\"\\n--- Evaluation Metrics Table ---\")\n",
    "        col_width = max(len(word) for row in metrics_table for word in row) + 2  # padding\n",
    "        for row in metrics_table:\n",
    "            print(\"\".join(word.ljust(col_width) for word in row))\n",
    "        \n",
    "        # Get category-wise AP\n",
    "        category_names = test_dataset.category_id_to_name\n",
    "        category_ap = []\n",
    "        print(\"\\n--- Category-wise AP@0.5 ---\")\n",
    "        cat_ap_table = [[\"Category\", \"AP@0.5\"]]\n",
    "        for idx, cat_id in enumerate(coco_eval.params.catIds):\n",
    "            if cat_id in category_names:\n",
    "                cat_name = category_names[cat_id]\n",
    "                # Get AP for this category\n",
    "                cat_ap = coco_eval.eval['precision'][0, :, idx, 0, 2].mean()\n",
    "                print(f\"{cat_name}: {cat_ap:.4f}\")\n",
    "                category_ap.append((cat_name, cat_ap))\n",
    "                cat_ap_table.append([cat_name, f\"{cat_ap:.4f}\"])\n",
    "        \n",
    "        # Print category AP table\n",
    "        print(\"\\n--- Category-wise AP@0.5 Table ---\")\n",
    "        col_width = max(len(word) for row in cat_ap_table for word in row) + 2  # padding\n",
    "        for row in cat_ap_table:\n",
    "            print(\"\".join(word.ljust(col_width) for word in row))\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        \n",
    "        # 1. IoU Threshold Metrics\n",
    "        plt.subplot(2, 2, 1)\n",
    "        iou_thresholds = ['IoU=0.50:0.95', 'IoU=0.50', 'IoU=0.75']\n",
    "        iou_values = [ap_mean, ap50, ap75]\n",
    "        bars1 = plt.bar(iou_thresholds, iou_values, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "        plt.title('Performance by IoU Threshold', fontsize=14)\n",
    "        plt.ylabel('AP Value', fontsize=12)\n",
    "        plt.ylim(0, max(iou_values) * 1.2)\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', fontsize=10)\n",
    "        \n",
    "        # 2. Object Size AP\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sizes = ['All', 'Small', 'Medium', 'Large']\n",
    "        size_values = [ap_mean, ap_small, ap_medium, ap_large]\n",
    "        # Replace -1 with 0 for visualization\n",
    "        size_values = [max(0, val) for val in size_values]\n",
    "        bars2 = plt.bar(sizes, size_values, color=['#3498db', '#95a5a6', '#2ecc71', '#e74c3c'])\n",
    "        plt.title('AP by Object Size', fontsize=14)\n",
    "        plt.ylabel('AP Value', fontsize=12)\n",
    "        plt.ylim(0, max(filter(lambda x: x != 0, size_values)) * 1.2)\n",
    "        for i, bar in enumerate(bars2):\n",
    "            height = bar.get_height()\n",
    "            if size_values[i] <= 0.001:\n",
    "                text = \"No objects\"\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., 0.01, text, ha='center', fontsize=8)\n",
    "            else:\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{size_values[i]:.3f}', ha='center', fontsize=10)\n",
    "        \n",
    "        # 3. AR by Max Detections\n",
    "        plt.subplot(2, 2, 3)\n",
    "        max_dets = ['maxDets=1', 'maxDets=10', 'maxDets=100']\n",
    "        ar_values = [ar_max1, ar_max10, ar_max100]\n",
    "        bars3 = plt.bar(max_dets, ar_values, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "        plt.title('Average Recall (AR) by Max Detections', fontsize=14)\n",
    "        plt.ylabel('AR Value', fontsize=12)\n",
    "        plt.ylim(0, max(ar_values) * 1.2)\n",
    "        for bar in bars3:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', fontsize=10)\n",
    "        \n",
    "        # 4. Category-wise AP\n",
    "        plt.subplot(2, 2, 4)\n",
    "        cat_names = [x[0] for x in category_ap]\n",
    "        cat_values = [x[1] for x in category_ap]\n",
    "        colors = ['#27ae60' if val > 0.3 else '#e74c3c' for val in cat_values]\n",
    "        bars4 = plt.bar(cat_names, cat_values, color=colors)\n",
    "        plt.title('Category-wise AP@0.5', fontsize=14)\n",
    "        plt.ylabel('AP Value', fontsize=12)\n",
    "        plt.ylim(0, max(cat_values) * 1.2 if cat_values else 0.1)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        for bar in bars4:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_metrics.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Finally, visualize predictions on sample images\n",
    "        visualize_predictions(test_dataset, model, device, num_samples=5)\n",
    "    else:\n",
    "        print(\"No detections above threshold. Cannot evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_dataset_main\n",
    "train_img_dir = os.path.join(path_to_dataset, \"train\")\n",
    "test_img_dir = os.path.join(path_to_dataset, \"test\")\n",
    "train_ann_file = os.path.join(train_img_dir, \"train.json\")\n",
    "test_ann_file = os.path.join(test_img_dir, \"test.json\")\n",
    "\n",
    "# Get number of classes from training dataset\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = len(CocoDataset(\n",
    "    root=train_img_dir,\n",
    "    annFile=train_ann_file\n",
    ").category_id_to_name) + 1  # +1 for background\n",
    "\n",
    "# Create model with same architecture and load the saved weights\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"maskrcnn_trained_1.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Now evaluate\n",
    "evaluate_model(model_output_path, test_img_dir, test_ann_file, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
