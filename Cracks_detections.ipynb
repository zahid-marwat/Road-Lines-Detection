{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving only segmented images\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    combined_mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)  # Initialize combined mask with 3 channels\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            masks = result.masks.data  # Segmentation masks\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)  # Get class IDs\n",
    "            class_names = result.names  # Get class names\n",
    "\n",
    "            for i, mask in enumerate(masks):\n",
    "                masks_detected = True  # Set flag to True if any mask is detected\n",
    "                mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "                mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Create a light bright green mask\n",
    "                light_bright_green = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "                light_bright_green[:, :] = (0, 255, 0)  # Light bright green color\n",
    "                mask_colored = cv2.bitwise_and(light_bright_green, light_bright_green, mask=mask_resized)\n",
    "                \n",
    "                combined_mask = cv2.add(combined_mask, mask_colored)  # Combine masks\n",
    "\n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                x1, y1, x2, y2 = result.boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "                center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (center_x, center_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    if masks_detected:\n",
    "        blended_image = cv2.addWeighted(blended_image, 0.8, combined_mask, 0.2, 0)  # Lower opacity of the mask\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_result.jpg\")\n",
    "        cv2.imwrite(output_path, blended_image)\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving segmented images along with bbox\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    combined_mask = np.zeros((img_height, img_width, 3), dtype=np.uint8)  # Initialize combined mask with 3 channels\n",
    "\n",
    "    yolo_format_lines = []  # List to store YOLOv8 format lines\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            masks = result.masks.data  # Segmentation masks\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)  # Get class IDs\n",
    "            class_names = result.names  # Get class names\n",
    "\n",
    "            for i, mask in enumerate(masks):\n",
    "                masks_detected = True  # Set flag to True if any mask is detected\n",
    "                mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "                mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Create a light bright green mask\n",
    "                light_bright_green = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "                light_bright_green[:, :] = (0, 255, 0)  # Light bright green color\n",
    "                mask_colored = cv2.bitwise_and(light_bright_green, light_bright_green, mask=mask_resized)\n",
    "                \n",
    "                combined_mask = cv2.add(combined_mask, mask_colored)  # Combine masks\n",
    "\n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                x1, y1, x2, y2 = result.boxes.xyxy[i].cpu().numpy().astype(int)\n",
    "                center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (center_x, center_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                # Convert to YOLOv8 format\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                yolo_center_x = (x1 + x2) / 2 / img_width\n",
    "                yolo_center_y = (y1 + y2) / 2 / img_height\n",
    "                yolo_width = bbox_width / img_width\n",
    "                yolo_height = bbox_height / img_height\n",
    "\n",
    "                # Get the segmentation points\n",
    "                contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                for contour in contours:\n",
    "                    points = contour.reshape(-1, 2)\n",
    "                    normalized_points = [(x / img_width, y / img_height) for x, y in points]\n",
    "                    formatted_points = \" \".join([f\"{x} {y}\" for x, y in normalized_points])\n",
    "                    yolo_format_lines.append(f\"{classes[i]} {formatted_points}\")\n",
    "\n",
    "    if masks_detected:\n",
    "        blended_image = cv2.addWeighted(blended_image, 0.8, combined_mask, 0.2, 0)  # Lower opacity of the mask\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}.jpg\")\n",
    "        # output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_result.jpg\")\n",
    "        cv2.imwrite(output_path, blended_image)\n",
    "\n",
    "        # Save YOLOv8 format annotations\n",
    "        txt_output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}.txt\")\n",
    "        with open(txt_output_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_format_lines))\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifted segmented Contours\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            masks = result.masks.data  # Segmentation masks\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)  # Get class IDs\n",
    "            class_names = result.names  # Get class names\n",
    "\n",
    "            for i, mask in enumerate(masks):\n",
    "                masks_detected = True  # Set flag to True if any mask is detected\n",
    "                mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "                mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Find contours of the mask\n",
    "                contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "                if contours:\n",
    "                    # Approximate contours to reduce the number of points\n",
    "                    approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                    \n",
    "                    for cnt in approx_contours:\n",
    "                        for j in range(len(cnt)):\n",
    "                            point = cnt[j][0]\n",
    "                            distance_from_center = abs(point[0] - img_width / 2)\n",
    "                            max_distance = img_width / 2\n",
    "                            shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                            if point[0] < img_width / 2:\n",
    "                                # Point is on the left side of the image\n",
    "                                shift_x = shift_factor\n",
    "                            else:\n",
    "                                # Point is on the right side of the image\n",
    "                                shift_x = -shift_factor\n",
    "\n",
    "                            shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                            M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                            shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                            cnt[j][0] = shifted_point\n",
    "\n",
    "                        # Connect endpoints of the contours to handle discontinuities\n",
    "                        for j in range(len(cnt) - 1):\n",
    "                            cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                        cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "                \n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (point[0], point[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    if masks_detected:\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_result.jpg\")\n",
    "        cv2.imwrite(output_path, blended_image)\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering overlapping boxes\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "overlap_threshold = 0.5  # Define the overlap threshold\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    filtered_results = []\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            class_names = result.names  # Get class names\n",
    "            # Filter out overlapping boxes\n",
    "            keep = np.ones(len(boxes), dtype=bool)\n",
    "            for i in range(len(boxes)):\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                for j in range(i + 1, len(boxes)):\n",
    "                    if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                        if confidences[i] > confidences[j]:\n",
    "                            keep[j] = False\n",
    "                        else:\n",
    "                            keep[i] = False\n",
    "\n",
    "            filtered_boxes = boxes[keep]\n",
    "            filtered_confidences = confidences[keep]\n",
    "            filtered_classes = classes[keep]\n",
    "            filtered_masks = result.masks.data[keep]\n",
    "\n",
    "            filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "    for boxes, confidences, classes, masks in filtered_results:\n",
    "        for i, mask in enumerate(masks):\n",
    "            masks_detected = True  # Set flag to True if any mask is detected\n",
    "            mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "            mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Approximate contours to reduce the number of points\n",
    "                approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                \n",
    "                for cnt in approx_contours:\n",
    "                    for j in range(len(cnt)):\n",
    "                        point = cnt[j][0]\n",
    "                        distance_from_center = abs(point[0] - img_width / 2)\n",
    "                        max_distance = img_width / 2\n",
    "                        shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                        if point[0] < img_width / 2:\n",
    "                            # Point is on the left side of the image\n",
    "                            shift_x = shift_factor\n",
    "                        else:\n",
    "                            # Point is on the right side of the image\n",
    "                            shift_x = -shift_factor\n",
    "\n",
    "                        shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                        shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                        cnt[j][0] = shifted_point\n",
    "\n",
    "                    # Connect endpoints of the contours to handle discontinuities\n",
    "                    for j in range(len(cnt) - 1):\n",
    "                        cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                    cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "            \n",
    "            class_name = class_names[classes[i]]\n",
    "            color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "            cv2.putText(\n",
    "                blended_image,\n",
    "                class_name,\n",
    "                (point[0], point[1]),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                color,\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    if masks_detected:\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}_result.jpg\")\n",
    "        cv2.imwrite(output_path, blended_image)\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Videos\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "input_video_path  = r\"C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\Data\\sample 2.mp4\"\n",
    "output_video_path = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\Data\\Detected\\images\\sample 2_resulted.mp4'\n",
    "\n",
    "\n",
    "# input_video_path  = r\"C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\Data\\SV_video_1727690479.0518215.mp4\"\n",
    "# output_video_path = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\Data\\Detected\\images\\SV_video_17276904790518215_resulted.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "overlap_threshold = 0.5  # Define the overlap threshold\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_height, img_width = frame.shape[:2]  # Get frame dimensions\n",
    "\n",
    "    results = model(frame, conf=0.3)\n",
    "\n",
    "    blended_image = frame.copy()  # Create a copy of the original frame for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    filtered_results = []\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            class_names = result.names  # Get class names\n",
    "\n",
    "            # Filter out overlapping boxes\n",
    "            keep = np.ones(len(boxes), dtype=bool)\n",
    "            for i in range(len(boxes)):\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                for j in range(i + 1, len(boxes)):\n",
    "                    if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                        if confidences[i] > confidences[j]:\n",
    "                            keep[j] = False\n",
    "                        else:\n",
    "                            keep[i] = False\n",
    "\n",
    "            filtered_boxes = boxes[keep]\n",
    "            filtered_confidences = confidences[keep]\n",
    "            filtered_classes = classes[keep]\n",
    "            filtered_masks = result.masks.data[keep]\n",
    "\n",
    "            filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "    for boxes, confidences, classes, masks in filtered_results:\n",
    "        for i, mask in enumerate(masks):\n",
    "            masks_detected = True  # Set flag to True if any mask is detected\n",
    "            mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "            mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Approximate contours to reduce the number of points\n",
    "                approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                \n",
    "                for cnt in approx_contours:\n",
    "                    for j in range(len(cnt)):\n",
    "                        point = cnt[j][0]\n",
    "                        distance_from_center = abs(point[0] - img_width / 2)\n",
    "                        max_distance = img_width / 2\n",
    "                        shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                        if point[0] < img_width / 2:\n",
    "                            # Point is on the left side of the image\n",
    "                            shift_x = shift_factor\n",
    "                        else:\n",
    "                            # Point is on the right side of the image\n",
    "                            shift_x = -shift_factor\n",
    "\n",
    "                        shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                        shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                        cnt[j][0] = shifted_point\n",
    "\n",
    "                    # Connect endpoints of the contours to handle discontinuities\n",
    "                    for j in range(len(cnt) - 1):\n",
    "                        cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                    cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "            \n",
    "            class_name = class_names[classes[i]]\n",
    "            color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "            cv2.putText(\n",
    "                blended_image,\n",
    "                class_name,\n",
    "                (point[0], point[1]),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                color,\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    if masks_detected:\n",
    "        out.write(blended_image)\n",
    "    else:\n",
    "        out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Annotations(with drawings)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi1 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "overlap_threshold = 0.5  # Define the overlap threshold\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    filtered_results = []\n",
    "    yolo_format_lines = []  # List to store YOLOv8 format lines\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            class_names = result.names  # Get class names\n",
    "            # Filter out overlapping boxes\n",
    "            keep = np.ones(len(boxes), dtype=bool)\n",
    "            for i in range(len(boxes)):\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                for j in range(i + 1, len(boxes)):\n",
    "                    if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                        if confidences[i] > confidences[j]:\n",
    "                            keep[j] = False\n",
    "                        else:\n",
    "                            keep[i] = False\n",
    "\n",
    "            filtered_boxes = boxes[keep]\n",
    "            filtered_confidences = confidences[keep]\n",
    "            filtered_classes = classes[keep]\n",
    "            filtered_masks = result.masks.data[keep]\n",
    "\n",
    "            filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "    for boxes, confidences, classes, masks in filtered_results:\n",
    "        for i, mask in enumerate(masks):\n",
    "            masks_detected = True  # Set flag to True if any mask is detected\n",
    "            mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "            mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Approximate contours to reduce the number of points\n",
    "                approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                \n",
    "                for cnt in approx_contours:\n",
    "                    for j in range(len(cnt)):\n",
    "                        point = cnt[j][0]\n",
    "                        distance_from_center = abs(point[0] - img_width / 2)\n",
    "                        max_distance = img_width / 2\n",
    "                        shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                        if point[0] < img_width / 2:\n",
    "                            # Point is on the left side of the image\n",
    "                            shift_x = shift_factor\n",
    "                        else:\n",
    "                            # Point is on the right side of the image\n",
    "                            shift_x = -shift_factor\n",
    "\n",
    "                        shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                        shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                        cnt[j][0] = shifted_point\n",
    "\n",
    "                    # Connect endpoints of the contours to handle discontinuities\n",
    "                    for j in range(len(cnt) - 1):\n",
    "                        cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                    cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "                \n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (point[0], point[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                # Convert to YOLOv8 format\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                yolo_center_x = (x1 + x2) / 2 / img_width\n",
    "                yolo_center_y = (y1 + y2) / 2 / img_height\n",
    "                yolo_width = bbox_width / img_width\n",
    "                yolo_height = bbox_height / img_height\n",
    "\n",
    "                # Get the segmentation points\n",
    "                for contour in contours:\n",
    "                    points = contour.reshape(-1, 2)\n",
    "                    normalized_points = [(x / img_width, y / img_height) for x, y in points]\n",
    "                    formatted_points = \" \".join([f\"{x} {y}\" for x, y in normalized_points])\n",
    "                    yolo_format_lines.append(f\"{classes[i]} {formatted_points}\")\n",
    "\n",
    "    if masks_detected:\n",
    "        output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}.jpg\")\n",
    "        cv2.imwrite(output_path, blended_image)\n",
    "\n",
    "        # Save YOLOv8 format annotations\n",
    "        txt_output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}.txt\")\n",
    "        with open(txt_output_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_format_lines))\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Annotations(with drawings)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from shutil import copyfile\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi1 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "output_dir = 'Data/Detected/images'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "overlap_threshold = 0.5  # Define the overlap threshold\n",
    "\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    filtered_results = []\n",
    "    yolo_format_lines = []  # List to store YOLOv8 format lines\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            class_names = result.names  # Get class names\n",
    "            # Filter out overlapping boxes\n",
    "            keep = np.ones(len(boxes), dtype=bool)\n",
    "            for i in range(len(boxes)):\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                for j in range(i + 1, len(boxes)):\n",
    "                    if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                        if confidences[i] > confidences[j]:\n",
    "                            keep[j] = False\n",
    "                        else:\n",
    "                            keep[i] = False\n",
    "\n",
    "            filtered_boxes = boxes[keep]\n",
    "            filtered_confidences = confidences[keep]\n",
    "            filtered_classes = classes[keep]\n",
    "            filtered_masks = result.masks.data[keep]\n",
    "\n",
    "            filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "    for boxes, confidences, classes, masks in filtered_results:\n",
    "        for i, mask in enumerate(masks):\n",
    "            masks_detected = True  # Set flag to True if any mask is detected\n",
    "            mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "            mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Approximate contours to reduce the number of points\n",
    "                approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                \n",
    "                for cnt in approx_contours:\n",
    "                    for j in range(len(cnt)):\n",
    "                        point = cnt[j][0]\n",
    "                        distance_from_center = abs(point[0] - img_width / 2)\n",
    "                        max_distance = img_width / 2\n",
    "                        shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                        if point[0] < img_width / 2:\n",
    "                            # Point is on the left side of the image\n",
    "                            shift_x = shift_factor\n",
    "                        else:\n",
    "                            # Point is on the right side of the image\n",
    "                            shift_x = -shift_factor\n",
    "\n",
    "                        shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                        shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                        cnt[j][0] = shifted_point\n",
    "\n",
    "                    # Convert to YOLOv8 format\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    bbox_width = x2 - x1\n",
    "                    bbox_height = y2 - y1\n",
    "                    yolo_center_x = (x1 + x2) / 2 / img_width\n",
    "                    yolo_center_y = (y1 + y2) / 2 / img_height\n",
    "                    yolo_width = bbox_width / img_width\n",
    "                    yolo_height = bbox_height / img_height\n",
    "\n",
    "                    # Get the segmentation points\n",
    "                    for contour in contours:\n",
    "                        points = contour.reshape(-1, 2)\n",
    "                        normalized_points = [(x / img_width, y / img_height) for x, y in points]\n",
    "                        formatted_points = \" \".join([f\"{x} {y}\" for x, y in normalized_points])\n",
    "                        yolo_format_lines.append(f\"{classes[i]} {formatted_points}\")\n",
    "\n",
    "    if masks_detected:\n",
    "        # Save YOLOv8 format annotations\n",
    "        txt_output_path = os.path.join(output_dir, f\"{os.path.splitext(image_name)[0]}.txt\")\n",
    "        with open(txt_output_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_format_lines))\n",
    "        \n",
    "        # Copy the original image to the output directory\n",
    "        copyfile(image_path, os.path.join(output_dir, image_name))\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LD_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
