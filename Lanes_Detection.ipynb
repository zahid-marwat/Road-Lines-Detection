{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 -From Json to YOLO (Roads' Lanes) for training \n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "class_mapping ={\n",
    "    'L10-1': 0,\n",
    "    'L18- 1': 1,\n",
    "    'L2-1': 2,\n",
    "    'L4-1': 3,\n",
    "    'L5-1': 4,\n",
    "    'L6-1b': 5,\n",
    "    'L7-2': 6,\n",
    "    'RGA-1': 7,\n",
    "    'RGA-4': 8,\n",
    "    'ZigZag Marking': 9\n",
    "}\n",
    "class_mapping_1 = {\n",
    "    \"L1-1a\": 0,\n",
    "    \"L1-1b\": 1,\n",
    "    \"L1-1c\": 2,\n",
    "    \"L2-1a\": 3,\n",
    "    \"L2-1b\": 4,\n",
    "    \"L2-1c\": 5,\n",
    "    \"L3-1a\": 6,\n",
    "    \"L3-1b\": 7,\n",
    "    \"L3-2a\": 8,\n",
    "    \"L3-2b\": 9,\n",
    "    \"L4-1a\": 10,\n",
    "    \"L4-1b\": 11,\n",
    "    \"L4-1c\": 12,\n",
    "    \"L4-1d\": 13,\n",
    "    \"L5-1a\": 14,\n",
    "    \"L5-1b\": 15,\n",
    "    \"L5-1c\": 16,\n",
    "    \"L6-1a\": 17,\n",
    "    \"L6-1b\": 18,\n",
    "    \"L6-1c\": 19,\n",
    "    \"L6-1d\": 20,\n",
    "    \"L7-1\": 21,\n",
    "    \"L7-2\": 22,\n",
    "    \"L8-1\": 23,\n",
    "    \"L9-1a\": 24,\n",
    "    \"L9-1b\": 25,\n",
    "    \"L9-1c\": 26,\n",
    "    \"L9-1d\": 27,\n",
    "    \"L10-1\": 28,\n",
    "    \"L11-1\": 29,\n",
    "    \"L18- 1\": 30,\n",
    "    \"L21-1\": 31,\n",
    "    \"L24-1\": 32,\n",
    "    \"L25-1\": 33,\n",
    "    \"L26-1\": 34,\n",
    "    \"L27-1\": 35,\n",
    "    \"L36-1a\": 36,\n",
    "    \"L36-1b\": 37,\n",
    "    \"L36-1c\": 38\n",
    "}\n",
    "\n",
    "def convert_to_yolo(json_path, output_dir, image_dir, class_mapping):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract image path\n",
    "    image_path = os.path.join(image_dir, os.path.basename(data['imagePath']))\n",
    "    \n",
    "    # Check if the image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: Image {image_path} not found. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Get image dimensions\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "    \n",
    "    # Process annotations in the \"shapes\" key\n",
    "    for annotation in data['shapes']:\n",
    "        label = annotation['label']\n",
    "        points = annotation['points']\n",
    "        \n",
    "        # Skip labels not in the mapping\n",
    "        if label not in class_mapping:\n",
    "            continue\n",
    "        class_id = class_mapping[label]\n",
    "        \n",
    "        # Normalize polygon points\n",
    "        normalized_points = [\n",
    "            [x / image_width, y / image_height] for x, y in points\n",
    "        ]\n",
    "        \n",
    "        # Flatten points into YOLOv8 format\n",
    "        flattened_points = \" \".join(f\"{x} {y}\" for x, y in normalized_points)\n",
    "        yolo_line = f\"{class_id} {flattened_points}\"\n",
    "        \n",
    "        # Write to label file\n",
    "        shutil.copy(image_path, output_dir)\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        label_file = os.path.join(output_dir, f\"{image_name}.txt\")\n",
    "        with open(label_file, 'a') as f:\n",
    "            f.write(yolo_line + \"\\n\")\n",
    "\n",
    "\n",
    "allfiles = glob.glob(r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\KSA dataset Insp\\Annotations' + os.sep + '*.json')\n",
    "random.shuffle(allfiles)\n",
    "\n",
    "# Split into 80% train and 20% val\n",
    "split_index = int(len(allfiles) * 0.8)\n",
    "train_files = allfiles[:split_index]\n",
    "val_files = allfiles[split_index:]\n",
    "\n",
    "# Define output directories\n",
    "train_output_dir = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\KSA dataset Insp\\final\\train'\n",
    "val_output_dir = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\KSA dataset Insp\\final\\val'\n",
    "image_dir = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\KSA dataset Insp\\Annotations'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "# Process training files\n",
    "for fileee in train_files:\n",
    "    convert_to_yolo(\n",
    "        json_path=fileee,\n",
    "        output_dir=train_output_dir,\n",
    "        image_dir=image_dir,\n",
    "        class_mapping=class_mapping\n",
    "    )\n",
    "\n",
    "# Process validation files\n",
    "for fileee in val_files:\n",
    "    convert_to_yolo(\n",
    "        json_path=fileee,\n",
    "        output_dir=val_output_dir,\n",
    "        image_dir=image_dir,\n",
    "        class_mapping=class_mapping\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP:2 For Annotations(with drawings and yolo_V8 annotations) for both images and videos\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob, os\n",
    "from shutil import copyfile\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi1 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "lines_model = 'Models/Lanes_seg_v2.pt'\n",
    "model = YOLO(lines_model)\n",
    "\n",
    "counter = 0\n",
    "images_src_dir = 'Data/Raw_images'\n",
    "images_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.jpg\")]\n",
    "videos_list = [i for i in glob.glob(images_src_dir + os.sep + \"*.mp4\")]\n",
    "annotated_output_dir = 'Data/Detected/annotated_images'\n",
    "original_output_dir = 'Data/Detected/original_images_with_annotations'\n",
    "\n",
    "os.makedirs(annotated_output_dir, exist_ok=True)\n",
    "os.makedirs(original_output_dir, exist_ok=True)\n",
    "\n",
    "colors = [\n",
    "    (255, 0, 0),  # Red\n",
    "    (255, 0, 0),  # Green\n",
    "    (0, 0, 255),  # Blue\n",
    "    (255, 255, 0),  # Cyan\n",
    "    (255, 0, 255),  # Magenta\n",
    "    (0, 255, 255),  # Yellow\n",
    "]\n",
    "\n",
    "overlap_threshold = 0.5  # Define the overlap threshold\n",
    "\n",
    "# Process images\n",
    "for imagefile in images_list:\n",
    "    image_name = os.path.basename(imagefile)\n",
    "    image_path = imagefile\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width = img.shape[:2]  # Get image dimensions\n",
    "\n",
    "    results = model(img, conf=0.3)\n",
    "\n",
    "    blended_image = img.copy()  # Create a copy of the original image for blending\n",
    "    masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "    filtered_results = []\n",
    "    yolo_format_lines = []  # List to store YOLOv8 format lines\n",
    "\n",
    "    for idx, result in enumerate(results):\n",
    "        if result.masks is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confidences = result.boxes.conf.cpu().numpy()\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            class_names = result.names  # Get class names\n",
    "            # Filter out overlapping boxes\n",
    "            keep = np.ones(len(boxes), dtype=bool)\n",
    "            for i in range(len(boxes)):\n",
    "                if not keep[i]:\n",
    "                    continue\n",
    "                for j in range(i + 1, len(boxes)):\n",
    "                    if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                        if confidences[i] > confidences[j]:\n",
    "                            keep[j] = False\n",
    "                        else:\n",
    "                            keep[i] = False\n",
    "\n",
    "            filtered_boxes = boxes[keep]\n",
    "            filtered_confidences = confidences[keep]\n",
    "            filtered_classes = classes[keep]\n",
    "            filtered_masks = result.masks.data[keep]\n",
    "\n",
    "            filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "    for boxes, confidences, classes, masks in filtered_results:\n",
    "        for i, mask in enumerate(masks):\n",
    "            masks_detected = True  # Set flag to True if any mask is detected\n",
    "            mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "            mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Find contours of the mask\n",
    "            contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                # Approximate contours to reduce the number of points\n",
    "                approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                \n",
    "                for cnt in approx_contours:\n",
    "                    for j in range(len(cnt)):\n",
    "                        point = cnt[j][0]\n",
    "                        distance_from_center = abs(point[0] - img_width / 2)\n",
    "                        max_distance = img_width / 2\n",
    "                        shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                        if point[0] < img_width / 2:\n",
    "                            # Point is on the left side of the image\n",
    "                            shift_x = shift_factor\n",
    "                        else:\n",
    "                            # Point is on the right side of the image\n",
    "                            shift_x = -shift_factor\n",
    "\n",
    "                        shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                        shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                        cnt[j][0] = shifted_point\n",
    "\n",
    "                    # Connect endpoints of the contours to handle discontinuities\n",
    "                    for j in range(len(cnt) - 1):\n",
    "                        cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                    cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "                \n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (point[0], point[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                # Convert to YOLOv8 format\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                yolo_center_x = (x1 + x2) / 2 / img_width\n",
    "                yolo_center_y = (y1 + y2) / 2 / img_height\n",
    "                yolo_width = bbox_width / img_width\n",
    "                yolo_height = bbox_height / img_height\n",
    "\n",
    "                # Get the segmentation points\n",
    "                for contour in contours:\n",
    "                    points = contour.reshape(-1, 2)\n",
    "                    normalized_points = [(x / img_width, y / img_height) for x, y in points]\n",
    "                    formatted_points = \" \".join([f\"{x} {y}\" for x, y in normalized_points])\n",
    "                    yolo_format_lines.append(f\"{classes[i]} {formatted_points}\")\n",
    "\n",
    "    if masks_detected:\n",
    "        # Save the annotated image\n",
    "        annotated_output_path = os.path.join(annotated_output_dir, f\"{os.path.splitext(image_name)[0]}.jpg\")\n",
    "        cv2.imwrite(annotated_output_path, blended_image)\n",
    "\n",
    "        # Save YOLOv8 format annotations\n",
    "        txt_output_path = os.path.join(original_output_dir, f\"{os.path.splitext(image_name)[0]}.txt\")\n",
    "        with open(txt_output_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_format_lines))\n",
    "        \n",
    "        # Copy the original image to the original output directory\n",
    "        copyfile(image_path, os.path.join(original_output_dir, image_name))\n",
    "        \n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"No masks detected for image: {image_name}\")\n",
    "\n",
    "# Process videos\n",
    "for videofile in videos_list:\n",
    "    video_name = os.path.basename(videofile)\n",
    "    input_video_path = videofile\n",
    "    output_video_path = os.path.join(annotated_output_dir, f\"{os.path.splitext(video_name)[0]}_resulted.mp4\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_counter = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img_height, img_width = frame.shape[:2]  # Get frame dimensions\n",
    "\n",
    "        results = model(frame, conf=0.3)\n",
    "\n",
    "        blended_image = frame.copy()  # Create a copy of the original frame for blending\n",
    "        masks_detected = False  # Flag to check if any masks were detected\n",
    "\n",
    "        filtered_results = []\n",
    "        yolo_format_lines = []  # List to store YOLOv8 format lines\n",
    "\n",
    "        for idx, result in enumerate(results):\n",
    "            if result.masks is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy().astype(int)\n",
    "                confidences = result.boxes.conf.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                class_names = result.names  # Get class names\n",
    "\n",
    "                # Filter out overlapping boxes\n",
    "                keep = np.ones(len(boxes), dtype=bool)\n",
    "                for i in range(len(boxes)):\n",
    "                    if not keep[i]:\n",
    "                        continue\n",
    "                    for j in range(i + 1, len(boxes)):\n",
    "                        if calculate_iou(boxes[i], boxes[j]) > overlap_threshold:\n",
    "                            if confidences[i] > confidences[j]:\n",
    "                                keep[j] = False\n",
    "                            else:\n",
    "                                keep[i] = False\n",
    "\n",
    "                filtered_boxes = boxes[keep]\n",
    "                filtered_confidences = confidences[keep]\n",
    "                filtered_classes = classes[keep]\n",
    "                filtered_masks = result.masks.data[keep]\n",
    "\n",
    "                filtered_results.append((filtered_boxes, filtered_confidences, filtered_classes, filtered_masks))\n",
    "\n",
    "        for boxes, confidences, classes, masks in filtered_results:\n",
    "            for i, mask in enumerate(masks):\n",
    "                masks_detected = True  # Set flag to True if any mask is detected\n",
    "                mask = (mask.cpu().numpy() * 255).astype(\"uint8\")  # Move tensor to CPU and convert to binary\n",
    "                mask_resized = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Find contours of the mask\n",
    "                contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "                if contours:\n",
    "                    # Approximate contours to reduce the number of points\n",
    "                    approx_contours = [cv2.approxPolyDP(cnt, epsilon=1.0, closed=True) for cnt in contours]\n",
    "                    \n",
    "                    for cnt in approx_contours:\n",
    "                        for j in range(len(cnt)):\n",
    "                            point = cnt[j][0]\n",
    "                            distance_from_center = abs(point[0] - img_width / 2)\n",
    "                            max_distance = img_width / 2\n",
    "                            shift_factor = (distance_from_center / max_distance) * 38  # Gradient shift\n",
    "\n",
    "                            if point[0] < img_width / 2:\n",
    "                                # Point is on the left side of the image\n",
    "                                shift_x = shift_factor\n",
    "                            else:\n",
    "                                # Point is on the right side of the image\n",
    "                                shift_x = -shift_factor\n",
    "\n",
    "                            shift_y = 0  # No vertical shift needed, adjust if necessary\n",
    "                            M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "                            shifted_point = cv2.transform(np.array([[point]], dtype=np.float32), M)[0][0]\n",
    "                            cnt[j][0] = shifted_point\n",
    "\n",
    "                        # Connect endpoints of the contours to handle discontinuities\n",
    "                        for j in range(len(cnt) - 1):\n",
    "                            cv2.line(blended_image, tuple(cnt[j][0]), tuple(cnt[j + 1][0]), (0, 255, 0), 1)\n",
    "                        cv2.line(blended_image, tuple(cnt[-1][0]), tuple(cnt[0][0]), (0, 255, 0), 1)\n",
    "                \n",
    "                class_name = class_names[classes[i]]\n",
    "                color = colors[classes[i] % len(colors)]  # Get color for the class\n",
    "\n",
    "                cv2.putText(\n",
    "                    blended_image,\n",
    "                    class_name,\n",
    "                    (point[0], point[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    color,\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "                # Convert to YOLOv8 format\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                yolo_center_x = (x1 + x2) / 2 / img_width\n",
    "                yolo_center_y = (y1 + y2) / 2 / img_height\n",
    "                yolo_width = bbox_width / img_width\n",
    "                yolo_height = bbox_height / img_height\n",
    "\n",
    "                # Get the segmentation points\n",
    "                for contour in contours:\n",
    "                    points = contour.reshape(-1, 2)\n",
    "                    normalized_points = [(x / img_width, y / img_height) for x, y in points]\n",
    "                    formatted_points = \" \".join([f\"{x} {y}\" for x, y in normalized_points])\n",
    "                    yolo_format_lines.append(f\"{classes[i]} {formatted_points}\")\n",
    "\n",
    "        if masks_detected:\n",
    "            out.write(blended_image)\n",
    "        else:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Save the original frame and YOLOv8 format annotations\n",
    "        if masks_detected:\n",
    "            frame_output_path = os.path.join(original_output_dir, f\"{os.path.splitext(video_name)[0]}_frame_{frame_counter}.jpg\")\n",
    "            cv2.imwrite(frame_output_path, frame)\n",
    "\n",
    "            txt_output_path = os.path.join(original_output_dir, f\"{os.path.splitext(video_name)[0]}_frame_{frame_counter}.txt\")\n",
    "            with open(txt_output_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(yolo_format_lines))\n",
    "\n",
    "        frame_counter += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 -Encoding images to base64 in JSON files + TXT to json and removing txt files\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "class_mapping = {\n",
    "    'L10-1': 0,\n",
    "    'L18- 1': 1,\n",
    "    'L2-1': 2,\n",
    "    'L4-1': 3,\n",
    "    'L5-1': 4,\n",
    "    'L6-1b': 5,\n",
    "    'L7-2': 6,\n",
    "    'RGA-1': 7,\n",
    "    'RGA-4': 8,\n",
    "    'ZigZag Marking': 9\n",
    "}\n",
    "\n",
    "# Define the additional keys and their values\n",
    "version = \"5.5.0\"\n",
    "flags = {}\n",
    "group_id = 346\n",
    "description = \"\"\n",
    "shape_type = \"polygon\"\n",
    "mask = None\n",
    "\n",
    "def yolo_to_json(yolo_data):\n",
    "    shapes = []\n",
    "    for data in yolo_data:\n",
    "        shape = {\n",
    "            \"label\": data[\"label\"],\n",
    "            \"points\": data[\"points\"],\n",
    "            \"group_id\": group_id,\n",
    "            \"description\": description,\n",
    "            \"shape_type\": shape_type,\n",
    "            \"flags\": flags,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "        shapes.append(shape)\n",
    "    \n",
    "    json_data = {\n",
    "        \"version\": version,\n",
    "        \"flags\": flags,\n",
    "        \"shapes\": shapes\n",
    "    }\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return encoded_string\n",
    "\n",
    "def convert_to_json(txt_path, output_dir, image_dir, class_mapping):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    image_name = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "    image_path = os.path.join(image_dir, f\"{image_name}.jpg\")\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: Image {image_path} not found. Skipping.\")\n",
    "        return\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "    \n",
    "    yolo_data = []\n",
    "    \n",
    "    # Process each line in the YOLOv8 format file\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id = int(parts[0])\n",
    "        points = list(map(float, parts[1:]))\n",
    "        \n",
    "        # Denormalize points\n",
    "        denormalized_points = [\n",
    "            [points[i] * image_width, points[i + 1] * image_height] for i in range(0, len(points), 2)\n",
    "        ]\n",
    "        \n",
    "        # Get label from class mapping\n",
    "        label = [k for k, v in class_mapping.items() if v == class_id][0]\n",
    "        \n",
    "        # Add annotation to YOLO data structure\n",
    "        yolo_data.append({\n",
    "            \"label\": label,\n",
    "            \"points\": denormalized_points\n",
    "        })\n",
    "    \n",
    "    # Convert YOLO data to JSON format\n",
    "    json_data = yolo_to_json(yolo_data)\n",
    "    json_data[\"imagePath\"] = image_name + \".jpg\"\n",
    "    \n",
    "    # Encode image to base64 and add to JSON data\n",
    "    encoded_image = encode_image_to_base64(image_path)\n",
    "    json_data[\"imageData\"] = encoded_image\n",
    "    \n",
    "    # Save JSON file\n",
    "    json_output_path = os.path.join(output_dir, f\"{image_name}.json\")\n",
    "    with open(json_output_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "    \n",
    "    # Remove the original txt file\n",
    "    os.remove(txt_path)\n",
    "\n",
    "files_dir = r'C:\\Users\\fbpza\\Desktop\\Road-Lines-Detection\\KSA dataset Insp\\final\\train'\n",
    "allfiles = glob.glob(files_dir + os.sep + '*.txt')\n",
    "\n",
    "# Process training files\n",
    "for fileee in allfiles:\n",
    "    convert_to_json(fileee, files_dir, files_dir, class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myyolov8testClone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
